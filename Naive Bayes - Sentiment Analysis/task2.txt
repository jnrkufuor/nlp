2.1 Using a test data set with only 18 lines might not be large enough to be representative. This could lead to an optimistic and high variance estimation of the model as you may not be able to determine how your model might behave with certain unseen words. Therefore it is hard to tell the overall generalized performance of the model. One way to improve on this would be to use all the data for training and testing through a k-fold cross-validation. We can then randomly assign some parts of the data set for training and some for testing and calculate the error rate on the test. This can be done 10 times with different randomly selected partitions. The main drawback to this is that since all the data is being used, the corpus would need to be blind. This can be a drawback because it may be useful at times to examine the data and be able to suggest possible features
2.2 In examining both predicted classes for my test set species, I realized that the red class predictions were more certain than the blues, with the reds ranging between -3000 and -8000 and the blues, between -7000 to -16000. The one misclassification ,for test set 1, which was a blue predicted as red, relative to the probabilities, was surprisingly not low in confidence. It had a probability of -3929, which when compared to the rest of the data set showed a high level of confidence.
2.3 I attempted to extend my model by looking at bigrams as features. This significantly increased my model's accuracy. With naive bayes, the assumption is that each word is independent but that is not always the case in English. Some words may follow others and bigrams bring that sense of co-dependence, thereby making the accuracy at prediction much better. The bigram had a very positive accuracy of 1 as compared to the former accuracy being at 0.94 for test data 1
